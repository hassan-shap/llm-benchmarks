{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hshapour/.pyenv/versions/3.10.12/envs/pytorch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading builder script: 100%|██████████| 30.7k/30.7k [00:00<00:00, 532kB/s]\n",
      "Downloading readme: 100%|██████████| 14.8k/14.8k [00:00<00:00, 4.35MB/s]\n",
      "Downloading data: 100%|██████████| 751k/751k [00:00<00:00, 1.50MB/s]\n",
      "Generating train split: 100%|██████████| 2490/2490 [00:00<00:00, 19977.92 examples/s]\n",
      "Generating validation split: 100%|██████████| 277/277 [00:00<00:00, 17620.19 examples/s]\n",
      "Generating test split: 100%|██████████| 3000/3000 [00:00<00:00, 22185.08 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "import json\n",
    "import torch\n",
    "from datasets.utils.logging import disable_progress_bar,enable_progress_bar\n",
    "\n",
    "dataset_name = \"super_glue\"  # The MMLU dataset name in Hugging Face Datasets\n",
    "dataset = load_dataset(dataset_name, name='rte')#[\"train\"]\n",
    "# print(dataset)\n",
    "\n",
    "labels = dataset[\"train\"].features[\"label\"]\n",
    "# def create_label_str(batch):\n",
    "#     return {\"label_str\": labels.int2str(batch[\"label\"])}\n",
    "def create_label_str(example):\n",
    "    return {\"label_str\": \"False\" if example[\"label\"]== 1 else \"True\"}\n",
    "# print(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['a'],\n",
      "    num_rows: 3\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "my_list = [{\"a\": 1}, {\"a\": 2}, {\"a\": 3}]\n",
    "dataset = Dataset.from_list(my_list)\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2490/2490 [00:00<00:00, 15748.06 examples/s]\n",
      "Map: 100%|██████████| 277/277 [00:00<00:00, 12406.40 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2490 277\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# data_train = dataset[\"train\"].map(create_label_str)   \n",
    "# data_test = dataset[\"validation\"].map(create_label_str)   \n",
    "# # print(d1)\n",
    "# # dd = d1.select(range(5)).shuffle()\n",
    "\n",
    "data_train = dataset[\"train\"].map(create_label_str)   \n",
    "data_test = dataset[\"validation\"].map(create_label_str)   \n",
    "\n",
    "# # d_all = concatenate_datasets([d1,d2])\n",
    "# data_train = d_all.select(range(5))\n",
    "# data_test = d_all.select(range(5,d_all.num_rows))\n",
    "\n",
    "print(data_train.num_rows, data_test.num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def json_writer(JSON_PATH, num_qs, n_shot_example,seed=10,space=False):\n",
    "    qlist = {}\n",
    "    # if n_shot_example >0:\n",
    "    #     instruction = 'The following are multiple choice questions (with answers) about  '\n",
    "    # else:\n",
    "    #     instruction = 'The following is a multiple choice question about  '\n",
    "    with open(JSON_PATH, 'w') as json_file:\n",
    "\n",
    "        for ind in range(num_qs):\n",
    "            dd_examples = data_train.select(range(5)).shuffle()\n",
    "            prompt = ''\n",
    "            for idx in range(n_shot_example):\n",
    "                prompt += \"Text: \" + dd_examples[idx]['premise'] + '\\n'\n",
    "                prompt += \"Hypothesis: \" + dd_examples[idx]['hypothesis'] + '\\n'\n",
    "                prompt += \"Entailment:\" + dd_examples[idx]['label_str'] + '\\n\\n'\n",
    "\n",
    "            prompt += \"Text: \" + data_test[ind]['premise'] + '\\n'\n",
    "            prompt += \"Hypothesis: \" + data_test[ind]['hypothesis'] + '\\n'\n",
    "            prompt += \"Entailment:\" \n",
    "    \n",
    "            \n",
    "            qlist['input'] = prompt\n",
    "            qlist['target_str'] = data_test[ind]['label_str']\n",
    "            qlist['target'] = (2**data_test[ind]['label']) % 2\n",
    "\n",
    "            # Serializing json\n",
    "            json_file.write(json.dumps(qlist) + '\\n')\n",
    "        # print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-shot example is being generated...\n",
      "1-shot example is being generated...\n",
      "2-shot example is being generated...\n",
      "3-shot example is being generated...\n",
      "4-shot example is being generated...\n",
      "5-shot example is being generated...\n"
     ]
    }
   ],
   "source": [
    "disable_progress_bar()\n",
    "\n",
    "# print(example_idx)\n",
    "n_example = data_test.num_rows\n",
    "n_shot_vals = [2]\n",
    "seed = 123\n",
    "for n_shot in range(6):\n",
    "# for n_shot in n_shot_vals:\n",
    "    print(f\"{n_shot}-shot example is being generated...\")\n",
    "    fname_json = f\"rte-data/{n_shot}_shot_examples.json\"\n",
    "    # fname_json = f\"boolq-data/{n_shot}_shot_examples_small.json\"\n",
    "    # JSON_eval = 'sample_eval.json'\n",
    "    json_writer(fname_json, n_example, n_shot, seed=seed, space= False)\n",
    "    # json_writer(JSON_eval,example_idx[(n_shot_example+1)*n_test_ex:])\n",
    "\n",
    "enable_progress_bar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 7476.48it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 1433.95it/s]\n",
      "Generating dev split: 3270 examples [00:00, 158649.11 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passage: Phantom pain -- Phantom pain sensations are described as perceptions that an individual experiences relating to a limb or an organ that is not physically part of the body. Limb loss is a result of either removal by amputation or congenital limb deficiency. However, phantom limb sensations can also occur following nerve avulsion or spinal cord injury.\n",
      "Question: is pain experienced in a missing body part or paralyzed area\n",
      "Answer:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# fname_json = f\"mmlu-data/ex_no_space.json\"\n",
    "n_shot = 0\n",
    "d1 = load_dataset(\"json\", data_files={\n",
    "        'dev' : f\"boolq-data/{n_shot}_shot_examples.json\"\n",
    "    })\n",
    "# print(d1['dev'])\n",
    "# print(d1['dev'].filter(lambda example: example[\"subject\"] == subject))\n",
    "# d2 = d1['dev'].filter(lambda example: example[\"subject\"] == subject)\n",
    "# # print(d2['input'][0][-1])\n",
    "# # print(\"---------\")\n",
    "print(d1['dev'][2]['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 6/6 [00:00<00:00, 17561.64it/s]\n",
      "Extracting data files: 100%|██████████| 6/6 [00:00<00:00, 1491.31it/s]\n",
      "Generating 0_shot split: 277 examples [00:00, 39765.28 examples/s]\n",
      "Generating 1_shot split: 277 examples [00:00, 53306.82 examples/s]\n",
      "Generating 2_shot split: 277 examples [00:00, 76465.86 examples/s]\n",
      "Generating 3_shot split: 277 examples [00:00, 46469.17 examples/s]\n",
      "Generating 4_shot split: 277 examples [00:00, 46118.70 examples/s]\n",
      "Generating 5_shot split: 277 examples [00:00, 35149.22 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "from collections import defaultdict\n",
    "\n",
    "fname_dict = {}\n",
    "for n_shot in range(6):\n",
    "    fname_dict[f\"{n_shot}_shot\"] = f\"rte-data/{n_shot}_shot_examples.json\"\n",
    "# data_all = defaultdict(DatasetDict)\n",
    "d_upload = load_dataset(\"json\", data_files=fname_dict)\n",
    "    # data_all[f\"n_shot = {n_shot}\"] = d1['dev']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 577.01ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.18s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 841.38ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.30it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 770.16ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.07it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 566.87ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:01<00:00,  1.07s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 661.04ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.12it/s]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 483.10ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:00<00:00,  1.08it/s]\n",
      "README.md: 100%|██████████| 1.33k/1.33k [00:00<00:00, 1.55MB/s]\n"
     ]
    }
   ],
   "source": [
    "# print(Dataset.from_dict(data_all))\n",
    "# Dataset.from_dict(data_all).push_to_hub(\"hassansh/boolq_n_shot\")\n",
    "d_upload.push_to_hub(\"hassansh/rte_n_shot\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text: Herceptin was already approved to treat the sickest breast cancer patients, and the company said, Monday, it will discuss with federal regulators the possibility of prescribing the drug for more breast cancer patients.\n",
      "Hypothesis: Herceptin can be used to treat breast cancer.\n",
      "Entailment:True\n",
      "\n",
      "Text: Judie Vivian, chief executive at ProMedica, a medical service company that helps sustain the 2-year-old Vietnam Heart Institute in Ho Chi Minh City (formerly Saigon), said that so far about 1,500 children have received treatment.\n",
      "Hypothesis: The previous name of Ho Chi Minh City was Saigon.\n",
      "Entailment:True\n",
      "\n",
      "Text: A man is due in court later charged with the murder 26 years ago of a teenager whose case was the first to be featured on BBC One's Crimewatch. Colette Aram, 16, was walking to her boyfriend's house in Keyworth, Nottinghamshire, on 30 October 1983 when she disappeared. Her body was later found in a field close to her home. Paul Stewart Hutchinson, 50, has been charged with murder and is due before Nottingham magistrates later.\n",
      "Hypothesis: Paul Stewart Hutchinson is accused of having stabbed a girl.\n",
      "Entailment:False\n",
      "\n",
      "Text: No Weapons of Mass Destruction Found in Iraq Yet.\n",
      "Hypothesis: Weapons of Mass Destruction Found in Iraq.\n",
      "Entailment:False\n",
      "\n",
      "Text: A place of sorrow, after Pope John Paul II died, became a place of celebration, as Roman Catholic faithful gathered in downtown Chicago to mark the installation of new Pope Benedict XVI.\n",
      "Hypothesis: Pope Benedict XVI is the new leader of the Roman Catholic Church.\n",
      "Entailment:True\n",
      "\n",
      "Text: Like the United States, U.N. officials are also dismayed that Aristide killed a conference called by Prime Minister Robert Malval in Port-au-Prince in hopes of bringing all the feuding parties together.\n",
      "Hypothesis: Aristide had Prime Minister Robert Malval  murdered in Port-au-Prince.\n",
      "Entailment:\n"
     ]
    }
   ],
   "source": [
    "print(d_upload['5_shot'][2]['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'passage', 'idx', 'label'],\n",
      "        num_rows: 9427\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['question', 'passage', 'idx', 'label'],\n",
      "        num_rows: 3270\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'passage', 'idx', 'label'],\n",
      "        num_rows: 3245\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
