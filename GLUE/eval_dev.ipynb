{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hshapour/.pyenv/versions/3.10.12/envs/pytorch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    logging,\n",
    ")\n",
    "\n",
    "import time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /home/hshapour/.pyenv/versions/3.10.12/envs/pytorch/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda115.so\n",
      "CUDA SETUP: CUDA runtime path found: /usr/lib/x86_64-linux-gnu/libcudart.so.11.0\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.0\n",
      "CUDA SETUP: Detected CUDA version 115\n",
      "CUDA SETUP: Loading binary /home/hshapour/.pyenv/versions/3.10.12/envs/pytorch/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda115.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:05<00:00,  2.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doing Llama tokenizer thingy\n",
      "pre-trained model's BOS EOS and PAD token id: 1 2 1  => It should be 1 2 None\n"
     ]
    }
   ],
   "source": [
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "# model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "cutoff_len = 4096\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "device_map = {\"\": 0}\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map,\n",
    "    # cache_dir = \"/dev/shm/hassan/.cahce/\"\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "if \"Llama-2\" in model_name or \"Mistral\" in model_name:\n",
    "    print(\"Doing Llama tokenizer thingy\")\n",
    "    # tokenizer.pad_token_id = tokenizer.bos_token_id\n",
    "    tokenizer.pad_token = tokenizer.bos_token\n",
    "\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "bos = tokenizer.bos_token_id\n",
    "eos = tokenizer.eos_token_id\n",
    "pad = tokenizer.pad_token_id\n",
    "print(\"pre-trained model's BOS EOS and PAD token id:\",bos,eos,pad,\" => It should be 1 2 None\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Passage: The Elder Scrolls Online -- As with other games in The Elder Scrolls series, the game is set on the continent of Tamriel. The events of the game occur a millennium before those of The Elder Scrolls V: Skyrim and around 800 years before The Elder Scrolls III: Morrowind and The Elder Scrolls IV: Oblivion. It has a broadly similar structure to Skyrim, with two separate conflicts progressing at the same time, one with the fate of the world in the balance, and one where the prize is supreme power on Tamriel. In The Elder Scrolls Online, the first struggle is against the Daedric Prince Molag Bal, who is attempting to meld the plane of Mundus with his realm of Coldharbour, and the second is to capture the vacant imperial throne, contested by three alliances of the mortal races. The player character has been sacrificed to Molag Bal, and Molag Bal has stolen their soul, the recovery of which is the primary game objective.\n",
      "Question: is elder scrolls online the same as skyrim\n",
      "Answer:False\n",
      "\n",
      "Passage: Windows Movie Maker -- Windows Movie Maker (formerly known as Windows Live Movie Maker in Windows 7) is a discontinued video editing software by Microsoft. It is a part of Windows Essentials software suite and offers the ability to create and edit videos as well as to publish them on OneDrive, Facebook, Vimeo, YouTube, and Flickr.\n",
      "Question: is windows movie maker part of windows essentials\n",
      "Answer:True\n",
      "\n",
      "Passage: Phantom pain -- Phantom pain sensations are described as perceptions that an individual experiences relating to a limb or an organ that is not physically part of the body. Limb loss is a result of either removal by amputation or congenital limb deficiency. However, phantom limb sensations can also occur following nerve avulsion or spinal cord injury.\n",
      "Question: is pain experienced in a missing body part or paralyzed area\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "# fname_json = f\"mmlu-data/ex_no_space.json\"\n",
    "n_shot = 2\n",
    "d1 = load_dataset(\"json\", data_files={\n",
    "        'dev' : f\"boolq-data/{n_shot}_shot_examples_small.json\"\n",
    "    })\n",
    "print(d1['dev'][2]['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch tensor elapsed time:  0.4870028495788574  sec\n"
     ]
    }
   ],
   "source": [
    "number_of_examples = 10\n",
    "\n",
    "################################################\n",
    "# method 2: generate tensor of examples\n",
    "ex1 = d1['dev'][:number_of_examples]['input']\n",
    "input_ids = tokenizer(ex1, padding=True, return_tensors='pt').input_ids\n",
    "input_ids = input_ids.to(device=0)\n",
    "\n",
    "tic = time.time()\n",
    "with torch.no_grad():\n",
    "    output = model(input_ids)\n",
    "    answers_2 = output.logits.squeeze()\n",
    "    if len(ex1) == 1:\n",
    "        answers_2 = answers_2.unsqueeze(0)\n",
    "\n",
    "toc = time.time()\n",
    "dt_2 = toc-tic\n",
    "\n",
    "print(\"Pytorch tensor elapsed time: \", dt_2, \" sec\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['True', 'False', '▁True', 'No']\n"
     ]
    }
   ],
   "source": [
    "top_k = 4\n",
    "\n",
    "ones = torch.ones_like(input_ids)\n",
    "last_token = input_ids == ones\n",
    "row_indices = torch.arange(input_ids.size(0))\n",
    "last_token = (torch.sum(last_token, dim = 1) + 1) * (-1)\n",
    "# if \"Llama-2\" in model_name:\n",
    "if \"Llama-2\" in model_name or \"Mistral\" in model_name:\n",
    "    last_token+=1\n",
    "_, top_choices2 = torch.topk(answers_2[row_indices,last_token,:], top_k)\n",
    "# _, top_choices2 = torch.topk(answers_2[row_indices,label_non_zero_id,:], top_k)\n",
    "print(tokenizer.convert_ids_to_tokens(top_choices2[0,:]))\n",
    "\n",
    "# print(torch.all(top_choices1[:,0] ==  top_choices2[:,0].cpu()))\n",
    "# print(top_choices1[:,0] ==  top_choices2[:,0].cpu())\n",
    "# print(tokenizer.decode(top_choices1[:,0]))\n",
    "# print(top_choices2[:,0].cpu())\n",
    "# print((top_choices3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [5852], 'attention_mask': [1]}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([5574, 8824, 5852, 3782], device='cuda:0')"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tokenizer(\"True\",add_special_tokens=False))\n",
    "top_choices2[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['▁choice', '1']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer.convert_ids_to_tokens(5852)\n",
    "toks = tokenizer('choice1').input_ids[1:]\n",
    "tokenizer.convert_ids_to_tokens(toks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
