{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hshapour/.pyenv/versions/3.10.12/envs/pytorch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading data: 100%|██████████| 44.0k/44.0k [00:00<00:00, 361kB/s]\n",
      "Generating train split: 100%|██████████| 400/400 [00:00<00:00, 13188.18 examples/s]\n",
      "Generating validation split: 100%|██████████| 100/100 [00:00<00:00, 10146.36 examples/s]\n",
      "Generating test split: 100%|██████████| 500/500 [00:00<00:00, 16046.89 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset, concatenate_datasets\n",
    "import json\n",
    "import torch\n",
    "from datasets.utils.logging import disable_progress_bar,enable_progress_bar\n",
    "\n",
    "dataset_name = \"super_glue\"  # The MMLU dataset name in Hugging Face Datasets\n",
    "dataset = load_dataset(dataset_name, 'copa')#[\"train\"]\n",
    "# print(dataset)\n",
    "\n",
    "labels = dataset[\"train\"].features[\"label\"]\n",
    "def create_label_str(batch):\n",
    "    return {\"label_str\": labels.int2str(batch[\"label\"])}\n",
    "# print(labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 495\n"
     ]
    }
   ],
   "source": [
    "\n",
    "d1 = dataset[\"train\"].map(create_label_str)   \n",
    "d2 = dataset[\"validation\"].map(create_label_str)   \n",
    "\n",
    "d_all = concatenate_datasets([d1,d2])\n",
    "data_train = d_all.select(range(5))\n",
    "data_test = d_all.select(range(5,d_all.num_rows))\n",
    "print(data_train.num_rows, data_test.num_rows)\n",
    "\n",
    "# print(data_test)\n",
    "# print(data_train)\n",
    "\n",
    "# dd = data_test.select(range(5)).shuffle()\n",
    "# print(data_train[4])\n",
    "# print(data_test[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def json_writer(JSON_PATH, num_qs, n_shot_example,seed=10,space=False):\n",
    "    qlist = {}\n",
    "    # if n_shot_example >0:\n",
    "    #     instruction = 'The following are multiple choice questions (with answers) about  '\n",
    "    # else:\n",
    "    #     instruction = 'The following is a multiple choice question about  '\n",
    "    with open(JSON_PATH, 'w') as json_file:\n",
    "\n",
    "        for ind in range(num_qs):\n",
    "            dd_examples = data_train.select(range(5)).shuffle()\n",
    "            prompt = ''\n",
    "            for idx in range(n_shot_example):\n",
    "                prompt += \"Premise: \" + dd_examples[idx]['premise'] + '\\n'\n",
    "                prompt += \"Question: \" + dd_examples[idx]['question'] + '\\n'\n",
    "                prompt += \"Alternative 1: \" + dd_examples[idx]['choice1'] + '\\n'\n",
    "                prompt += \"Alternative 2: \" + dd_examples[idx]['choice2'] + '\\n'\n",
    "                prompt += \"Answer:\" + str(dd_examples[idx]['label']+1) + '\\n\\n'\n",
    "\n",
    "            prompt += \"Premise: \" + data_test[ind]['premise'] + '\\n'\n",
    "            prompt += \"Question: \" + data_test[ind]['question'] + '\\n'\n",
    "            prompt += \"Alternative 1: \" + data_test[ind]['choice1'] + '\\n'\n",
    "            prompt += \"Alternative 2: \" + data_test[ind]['choice2'] + '\\n'\n",
    "            prompt += \"Answer:\"\n",
    "    \n",
    "            qlist['input'] = prompt\n",
    "            qlist['target_str'] = data_test[ind]['label_str']\n",
    "            qlist['target'] = data_test[ind]['label']\n",
    "\n",
    "            # Serializing json\n",
    "            json_file.write(json.dumps(qlist) + '\\n')\n",
    "        # print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0-shot example is being generated...\n",
      "1-shot example is being generated...\n",
      "2-shot example is being generated...\n",
      "3-shot example is being generated...\n",
      "4-shot example is being generated...\n",
      "5-shot example is being generated...\n"
     ]
    }
   ],
   "source": [
    "disable_progress_bar()\n",
    "\n",
    "# print(example_idx)\n",
    "n_example = data_test.num_rows\n",
    "n_shot_vals = [2]\n",
    "seed = 123\n",
    "for n_shot in range(6):\n",
    "# for n_shot in n_shot_vals:\n",
    "    print(f\"{n_shot}-shot example is being generated...\")\n",
    "    fname_json = f\"copa-data/{n_shot}_shot_examples.json\"\n",
    "    # fname_json = f\"boolq-data/{n_shot}_shot_examples_small.json\"\n",
    "    # JSON_eval = 'sample_eval.json'\n",
    "    json_writer(fname_json, n_example, n_shot, seed=seed, space= False)\n",
    "    # json_writer(JSON_eval,example_idx[(n_shot_example+1)*n_test_ex:])\n",
    "\n",
    "enable_progress_bar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Premise: The runner wore shorts.\n",
      "Question: cause\n",
      "Alternative 1: The forecast predicted high temperatures.\n",
      "Alternative 2: She planned to run along the beach.\n",
      "Answer:1\n",
      "\n",
      "Premise: My body cast a shadow over the grass.\n",
      "Question: cause\n",
      "Alternative 1: The sun was rising.\n",
      "Alternative 2: The grass was cut.\n",
      "Answer:1\n",
      "\n",
      "Premise: The man got a discount on his groceries.\n",
      "Question: cause\n",
      "Alternative 1: He greeted the cashier.\n",
      "Alternative 2: He used a coupon.\n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "# fname_json = f\"mmlu-data/ex_no_space.json\"\n",
    "n_shot = 2\n",
    "d1 = load_dataset(\"json\", data_files={\n",
    "        'dev' : f\"copa-data/{n_shot}_shot_examples.json\"\n",
    "    })\n",
    "# print(d1['dev'])\n",
    "# print(d1['dev'].filter(lambda example: example[\"subject\"] == subject))\n",
    "# d2 = d1['dev'].filter(lambda example: example[\"subject\"] == subject)\n",
    "# # print(d2['input'][0][-1])\n",
    "# # print(\"---------\")\n",
    "print(d1['dev'][2]['input'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 6/6 [00:00<00:00, 22509.68it/s]\n",
      "Extracting data files: 100%|██████████| 6/6 [00:00<00:00, 1607.32it/s]\n",
      "Generating 0_shot split: 495 examples [00:00, 113545.56 examples/s]\n",
      "Generating 1_shot split: 495 examples [00:00, 155951.36 examples/s]\n",
      "Generating 2_shot split: 495 examples [00:00, 137422.59 examples/s]\n",
      "Generating 3_shot split: 495 examples [00:00, 115638.88 examples/s]\n",
      "Generating 4_shot split: 495 examples [00:00, 67225.12 examples/s]\n",
      "Generating 5_shot split: 495 examples [00:00, 61525.57 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import DatasetDict, Dataset\n",
    "from collections import defaultdict\n",
    "\n",
    "fname_dict = {}\n",
    "for n_shot in range(6):\n",
    "    fname_dict[f\"{n_shot}_shot\"] =  f\"copa-data/{n_shot}_shot_examples.json\"\n",
    "\n",
    "# data_all = defaultdict(DatasetDict)\n",
    "d_upload = load_dataset(\"json\", data_files=fname_dict)\n",
    "    # data_all[f\"n_shot = {n_shot}\"] = d1['dev']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 482.16ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:11<00:00, 11.97s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 787.07ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:12<00:00, 12.60s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 322.24ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:10<00:00, 10.82s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 579.88ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:06<00:00,  6.16s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 554.22ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:03<00:00,  3.95s/it]\n",
      "Creating parquet from Arrow format: 100%|██████████| 1/1 [00:00<00:00, 522.46ba/s]\n",
      "Uploading the dataset shards: 100%|██████████| 1/1 [00:02<00:00,  2.45s/it]\n",
      "README.md: 100%|██████████| 1.03k/1.03k [00:00<00:00, 1.54MB/s]\n"
     ]
    }
   ],
   "source": [
    "# print(Dataset.from_dict(data_all))\n",
    "# Dataset.from_dict(data_all).push_to_hub(\"hassansh/boolq_n_shot\")\n",
    "d_upload.push_to_hub(\"hassansh/copa_n_shot\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
