{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/envs/mytorch/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/targets/x86_64-linux/lib/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /opt/conda/envs/mytorch/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/mytorch/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /opt/conda/envs/mytorch did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4a4f947593c41cda23ffe6bcc9596b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-trained model's BOS EOS and PAD token id: 1 2 1  => It should be 1 2 None\n"
     ]
    }
   ],
   "source": [
    "# model_name = \"/data/opt-350m\"\n",
    "model_name = \"/data/llama-hf/Llama-2-7b-hf\"\n",
    "# model_name = \"/data/Mistral-7B-Instruct-v0.2\"\n",
    "# model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=False,\n",
    ")\n",
    "device_map = {\"\": 0}\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "if \"Llama-2\" in model_name or \"Mistral\" in model_name:\n",
    "    tokenizer.pad_token = tokenizer.bos_token\n",
    "\n",
    "bos = tokenizer.bos_token_id\n",
    "eos = tokenizer.eos_token_id\n",
    "pad = tokenizer.pad_token_id\n",
    "print(\"pre-trained model's BOS EOS and PAD token id:\",bos,eos,pad,\" => It should be 1 2 None\")\n",
    "\n",
    "##### tokenizer.pad_token_id = 0  # unk. we want this to be different from the eos token\n",
    "tokenizer.padding_side = \"right\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['question', 'passage', 'idx', 'label'],\n",
      "        num_rows: 9427\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['question', 'passage', 'idx', 'label'],\n",
      "        num_rows: 3270\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['question', 'passage', 'idx', 'label'],\n",
      "        num_rows: 3245\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset_name = \"super_glue\"  # The MMLU dataset name in Hugging Face Datasets\n",
    "dataset = load_dataset(dataset_name, 'boolq')#[\"train\"]\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassLabel(names=['False', 'True'], id=None)\n",
      "Dataset({\n",
      "    features: ['question', 'passage', 'idx', 'label', 'label_str'],\n",
      "    num_rows: 9427\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# for split in dataset:\n",
    "#     print(split)\n",
    "labels = dataset[\"train\"].features[\"label\"]\n",
    "def create_label_str(batch):\n",
    "    return {\"label_str\": labels.int2str(batch[\"label\"])}\n",
    "print(labels)\n",
    "\n",
    "d1 = dataset[\"train\"].map(create_label_str)   \n",
    "print(d1)\n",
    "\n",
    "# idx = 0\n",
    "# # print(dataset[idx]['sentence1'])\n",
    "# # print(dataset[idx]['sentence2'])\n",
    "# # print(dataset[idx]['label'])\n",
    "# print(dataset['question'][:3])\n",
    "# print(create_label_str(dataset[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "prompt = \"\"\n",
    "for idx in range(6):\n",
    "    # prompt = \"Are the following two sentences consistent? Yes or No?\\n\\n\"\n",
    "    prompt += \"Passage: \" + d1[idx]['passage'] + '\\n'\n",
    "    prompt += \"Question: \" + d1[idx]['question'] + '\\n'\n",
    "    # prompt += \"Based on the passage, \" + dataset[idx]['question'] + \"? Yes or No?\" + '\\n'\n",
    "    if idx == 5:\n",
    "        prompt += \"Answer:\"\n",
    "    else:\n",
    "        prompt += \"Answer:\" + d1[idx]['label_str'] + '\\n\\n'\n",
    "\n",
    "\n",
    "# # print(prompt)\n",
    "# pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length= 200)\n",
    "# result = pipe(f\"<s>[INST] {prompt} [/INST]\")\n",
    "# print(result[0]['generated_text'])\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors='pt').input_ids\n",
    "input_ids = input_ids.to(device=0)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_ids)\n",
    "    answers = output.logits.squeeze()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(torch.argmax(answers[-1])))\n",
    "print(d1[5]['label_str'])\n",
    "# print(input_ids.size(),answers.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['‚ñÅFalse']\n"
     ]
    }
   ],
   "source": [
    "s = tokenizer('False', add_special_tokens=False)\n",
    "print(tokenizer.convert_ids_to_tokens(s.input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
