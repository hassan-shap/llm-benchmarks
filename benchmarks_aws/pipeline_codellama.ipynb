{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5c2742f14e7483499e8854eb8ca927c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.05k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca795683f4c5442da4ca43a18d6eb2b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin.index.json:   0%|          | 0.00/36.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c050eb5d3f7a44efa536e711f43284ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae547f346bd84c969df2fc131f8b6bed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00007.bin:   0%|          | 0.00/9.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f67dead21b74ccdaef3071d1311a311",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00007.bin:   0%|          | 0.00/9.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24e751267920491f9f0e43a5a5ee58f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00003-of-00007.bin:   0%|          | 0.00/9.85G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d904b6b42dcd4c8284fe00bbf88c8a8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00004-of-00007.bin:   0%|          | 0.00/9.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9fd5239528734370894922eb74e8fa54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00005-of-00007.bin:   0%|          | 0.00/9.85G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc98b520b462460783e16f24cc4b4c7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00006-of-00007.bin:   0%|          | 0.00/9.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96df56e8313e4c4c958d04abd1105ae3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00007-of-00007.bin:   0%|          | 0.00/4.08G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /opt/conda/envs/mytorch/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so\n",
      "CUDA SETUP: CUDA runtime path found: /usr/local/cuda/targets/x86_64-linux/lib/libcudart.so\n",
      "CUDA SETUP: Highest compute capability among GPUs detected: 8.6\n",
      "CUDA SETUP: Detected CUDA version 118\n",
      "CUDA SETUP: Loading binary /opt/conda/envs/mytorch/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda118.so...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/mytorch/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /opt/conda/envs/mytorch did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7392797133a84ddcb172e098ea69a701",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/7 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b14b947c38e4763b899e5d59ad41cd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/111 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09d9313b2abf4e2e8c1c65001a29c697",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/677 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edf3f2c80c6848daa5292d84b5e31c44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/777k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "544e6a6fafc148e984765b63f24e858b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/442k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2f2a940f368446e94901102ee224421",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.06M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "311a85973e8447d4abba581073f6281e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/532 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pre-trained model's BOS EOS and PAD token id: 0 0 None  => It should be 1 2 None\n"
     ]
    }
   ],
   "source": [
    "# model_name = \"/data/opt-350m\"\n",
    "# model_name = \"/data/llama-hf/Llama-2-7b-hf\"\n",
    "# model_name = \"/data/Mistral-7B-Instruct-v0.2\"\n",
    "# model_name = \"mistralai/Mistral-7B-v0.1\"\n",
    "model_name = \"codellama/CodeLlama-13b-Python-hf\"\n",
    "model_name = \"bigcode/starcoder\"\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    ")\n",
    "device_map = {\"\": 0}\n",
    "# Load model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=device_map\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "model.config.use_cache = False\n",
    "model.config.pretraining_tp = 1\n",
    "\n",
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "if \"Llama-2\" in model_name or \"Mistral\" in model_name:\n",
    "    tokenizer.pad_token = tokenizer.bos_token\n",
    "\n",
    "bos = tokenizer.bos_token_id\n",
    "eos = tokenizer.eos_token_id\n",
    "pad = tokenizer.pad_token_id\n",
    "print(\"pre-trained model's BOS EOS and PAD token id:\",bos,eos,pad,\" => It should be 1 2 None\")\n",
    "\n",
    "##### tokenizer.pad_token_id = 0  # unk. we want this to be different from the eos token\n",
    "tokenizer.padding_side = \"right\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def quicksort(a: List(int)) -> List:\n",
      "    \"\"\" Quicksort algorithm.\n",
      "\n",
      "    Args:\n",
      "        a (List(int)): List to be sorted.\n",
      "\n",
      "    Returns:\n",
      "        List: Sorted list.\n",
      "    \"\"\"\n",
      "    if len(a) <= 1:\n",
      "        return a\n",
      "    pivot = a[0]\n",
      "    less = [x for x in a[1:] if x < pivot]\n",
      "    greater = [x for x in a[1:] if x >= pivot]\n",
      "    result = quicksort(less) + [pivot] + quicksort(greater)\n",
      "    return result\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# prompt = \"def fibonoacci(\"\n",
    "# prompt = '''def quicksort(a: List(int)) -> List:\n",
    "#     \"\"\" <FILL_ME>\n",
    "#     return result\n",
    "# '''\n",
    "\n",
    "prompt = '''def SwapGate(q1,q2) -> List:\n",
    "    \"\"\" <FILL_ME>\n",
    "    return result\n",
    "'''\n",
    "# print(prompt)\n",
    "# pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length= 300)\n",
    "# result = pipe(f\"{prompt}\")\n",
    "# print(result[0]['generated_text'])\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].to(\"cuda\")\n",
    "output = model.generate(\n",
    "    input_ids,\n",
    "    max_new_tokens=200,\n",
    ")\n",
    "output = output[0].to(\"cpu\")\n",
    "\n",
    "filling = tokenizer.decode(output[input_ids.shape[1]:], skip_special_tokens=True)\n",
    "print(prompt.replace(\"<FILL_ME>\", filling))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> [INST] <<SYS>>\\nProvide answers in Python\\n<</SYS>>\\n\\nDesign a quantum circuit to teleport q2 to q1.[/INST]\n",
      "\n",
      "[TITLE]Quantum Teleportation[/TITLE]\n",
      "\n",
      "[SYNTAX]\n",
      "\n",
      "[CODE]\n",
      "\n",
      "from qiskit import QuantumCircuit, ClassicalRegister, QuantumRegister\n",
      "from qiskit import execute\n",
      "from qiskit import Aer\n",
      "\n",
      "q = QuantumRegister(3)\n",
      "c = ClassicalRegister(3)\n",
      "qc = QuantumCircuit(q, c)\n",
      "\n",
      "qc.h(q[1])\n",
      "qc.cx(q[1], q[2])\n",
      "qc.cx(q[1], q[0])\n",
      "qc.h(q[0])\n",
      "qc.measure(q[0], c[0])\n",
      "qc.measure(q[1], c[1])\n",
      "qc.z(q[2]).c_if(c, 2)\n",
      "qc.measure(q[2], c[2])\n",
      "\n",
      "backend_sim = Aer.get_backend('qasm_simulator')\n",
      "job_sim = execute(qc, backend_sim)\n",
      "result_sim = job_sim.result()\n",
      "\n",
      "print(result_sim.get_counts(qc))\n",
      "\n",
      "[/CODE]\n",
      "\n",
      "[/SYNTAX]\n",
      "\n",
      "[EXPLAIN]\n",
      "\n",
      "[/EXPLAIN]\n",
      "\n",
      "[\n"
     ]
    }
   ],
   "source": [
    "# prompt = 'def remove_non_ascii(s: str) -> str:\\n    \"\"\" '\n",
    "system = \"Provide answers in Python\"\n",
    "# user = \"Write a quantum circuit to swap two qubits using qiskit.\"\n",
    "user = \"Design a quantum circuit to teleport q2 to q1.\"\n",
    "\n",
    "prompt = f\"<s>[INST] <<SYS>>\\\\n{system}\\\\n<</SYS>>\\\\n\\\\n{user}[/INST]\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(\"cuda\")\n",
    "\n",
    "# inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "output = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_new_tokens=300,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    temperature=0.1,\n",
    ")\n",
    "output = output[0].to(\"cpu\")\n",
    "print(tokenizer.decode(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import cudaq \n",
      "\n",
      " def teleport_measure(q0,q1,q2) :\n",
      "  q0.H()\n",
      "  q1.H()\n",
      "  q2.H()\n",
      "  q0.cnot(q1)\n",
      "  q1.cnot(q2)\n",
      "  q2.cnot(q0)\n",
      "  q0.H()\n",
      "  q1.H()\n",
      "  q2.H()\n",
      "  return q0.measure()\n",
      "\n",
      " def teleport_measure_all(q0,q1,q2) :\n",
      "  q0.H()\n",
      "  q1.H()\n",
      "  q2.H()\n",
      "  q0.cnot(q1)\n",
      "  q1.cnot(q2)\n",
      "  q2.cnot(q0)\n",
      "  q0.H()\n",
      "  q1.H()\n",
      "  q2.H()\n",
      "  return q0.measure(),q1.measure(),q2.measure()\n",
      "\n",
      " def teleport_measure_all_q(q0,q1,q2) :\n",
      "  q0.H()\n",
      "  q1.H()\n",
      "  q2.H()\n",
      "  q0.cnot(q1)\n",
      "  q1.cnot(q2)\n",
      "  q2.cnot(q0)\n",
      "  q0.H()\n",
      "  q1.H()\n",
      "  q2.H()\n",
      "  return q0,q1,q2\n",
      "\n",
      " def teleport_measure_all_q_all(q0,q1,q2) :\n",
      "  q0.\n"
     ]
    }
   ],
   "source": [
    "# # prompt = 'def remove_non_ascii(s: str) -> str:\\n    \"\"\" '\n",
    "# system = \"Provide answers in Python\"\n",
    "# # user = \"Write a quantum circuit to swap two qubits using qiskit.\"\n",
    "# user = \"Design a quantum circuit to teleport q2 to q1.\"\n",
    "\n",
    "# prompt = f\"<s>[INST] <<SYS>>\\\\n{system}\\\\n<</SYS>>\\\\n\\\\n{user}[/INST]\"\n",
    "prompt = \"# function to generate a quantum circuit to do quantum teleportation from q0 to q2:\\n\"\n",
    "prompt = \"import cudaq \\n\\n def teleport_measure(q0,q1,q2)\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False).to(\"cuda\")\n",
    "\n",
    "# inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "output = model.generate(\n",
    "    inputs[\"input_ids\"],\n",
    "    max_new_tokens=300,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    temperature=0.1,\n",
    ")\n",
    "output = output[0].to(\"cpu\")\n",
    "print(tokenizer.decode(output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/mytorch/lib/python3.10/site-packages/transformers/generation/utils.py:1473: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use and modify the model generation configuration (see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import cudaq\n",
      "from cudaq.cuda import Cuda\n",
      "from cudaq.cudart import CudaRuntime\n",
      "from cudaq.cublas import Cublas\n",
      "from cudaq.cufft import Cufft\n",
      "from cudaq.cufft import CufftPlan1d\n",
      "from cudaq.cufft import CufftPlan2d\n",
      "from cudaq.cufft import CufftPlan3d\n",
      "from cudaq.cufft import CufftPlanNd\n",
      "from cudaq.cufft import CufftPlanMany\n",
      "from cudaq.cufft import CufftExec\n",
      "from cudaq.cufft import CufftExecC2C\n",
      "from cudaq.cufft import CufftExecR2C\n",
      "from cudaq.cufft import CufftExecC2R\n",
      "from cudaq.c\n"
     ]
    }
   ],
   "source": [
    "# prompt = \"Write a quantum circuit to swap two qubits.\"\n",
    "# prompt = \"def SwapGate(q1, q2):\"\n",
    "prompt = \"import cudaq\"\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens= 200)\n",
    "result = pipe(f\"{prompt}\")\n",
    "print(result[0]['generated_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
